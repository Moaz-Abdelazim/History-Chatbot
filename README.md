
# ğŸ›ï¸ History-Chatbot

A retrieval-augmented chatbot that answers questions about famous historical events, figures, and places across six countries. This project uses a combination of local language models (Qwen series) and vector search databases (FAISS or Weaviate) to deliver intelligent, context-aware answers.

---

## ğŸ“Œ Description

**History-Chatbot** is a RAG (Retrieval-Augmented Generation) system built to answer history-related questions using information sourced from Wikipedia. It supports user-configurable options such as:

- Choice of local LLMs: **Qwen 0.6B** or **Qwen 1.8B**
- Retrieval of a custom number of text chunks
- Selection of vector store: **FAISS** or **Weaviate**

---

## ğŸŒ Supported Countries

The chatbot currently supports historical data related to the following countries:

- ğŸ‡¨ğŸ‡³ China  
- ğŸ‡ªğŸ‡¬ Egypt  
- ğŸ´ England  
- ğŸ‡«ğŸ‡· France  
- ğŸ‡©ğŸ‡ª Germany  
- ğŸ‡¬ğŸ‡· Greece  

---

## ğŸ“š Dataset

Historical data is sourced from a Kaggle dataset:  
[ğŸ“ History from Wikipedia](https://www.kaggle.com/datasets/budibudi/history-from-wikipedia)

---

## ğŸš€ Features

- Lightweight and local â€” no external API calls
- Switch between small Qwen models based on performance needs
- Choose between FAISS (local) or Weaviate (cloud/local) vector database
- Streamlit-powered frontend for a simple UI
- Modular codebase for easy extension

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Install Dependencies

> âœ… **Python 3.11 is required**  
> Make sure you have Python 3.11 installed before proceeding.

```bash
git clone https://github.com/Moaz-Abdelazim/History-Chatbot.git
cd History-Chatbot
pip install -r requirements.txt
```

> ğŸ’¡ You can create a virtual environment (optional but recommended):
```bash
python3.11 -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

### 2ï¸âƒ£ Download the LLMs

```bash
python download_model.py
```

This will fetch the Qwen models used in the inference pipeline.

### 3ï¸âƒ£ Run the Application

```bash
streamlit run app.py --server.fileWatcherType none
```

---

## ğŸ’¡ Example Query

> **Question**: *"What events led to the unification of Germany?"*  
> **Expected Output**: The chatbot will retrieve relevant chunks and summarize an answer using the selected model.

---

## ğŸ§  File Structure Overview

```
History-Chatbot/
â”œâ”€â”€ app.py                  # Streamlit UI
â”œâ”€â”€ create_llm_model.py     # Model instantiation logic
â”œâ”€â”€ download_model.py       # Downloads Qwen models
â”œâ”€â”€ inference_engine.py     # Handles inference
â”œâ”€â”€ prompt_optimizer.py     # Refines user queries
â”œâ”€â”€ query_handler.py        # Full RAG pipeline logic
â”œâ”€â”€ search.py               # Chunk retrieval
â”œâ”€â”€ Weaviate_Client.py      # Weaviate vector DB interface
â”œâ”€â”€ ImportData.py           # Loads and processes dataset
â”œâ”€â”€ README.md               # Project overview
â””â”€â”€ requirements.txt        # Project dependencies
```

---

## âœ¨ Future Enhancements

- âœ… Add support for more countries and regions  
- ğŸ” Use larger or more accurate language models with quantization or GPU acceleration  
- ğŸ“Š Integrate answer evaluation metrics (e.g., semantic similarity, factual accuracy)  
- ğŸ–¼ï¸ Improve UI using Streamlit components like dropdowns, expandable sections, and history view  
- ğŸ§ª Add unit tests and CI/CD integration  

---

## ğŸ“¬ Contact

Developed by [**Moaz Abdelazim**](https://github.com/Moaz-Abdelazim)  
Feel free to open issues or pull requests with suggestions, bugs, or contributions!

---
